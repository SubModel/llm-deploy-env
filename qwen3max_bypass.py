# llm_proxy.py - LLM è¯·æ±‚ä»£ç†æœåŠ¡ï¼ˆå…³é”®è¯è¿‡æ»¤å’Œæ›¿æ¢ï¼‰
import json
import os
import re
import asyncio
import httpx
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse, StreamingResponse
import uvicorn
import argparse

# é…ç½®
BACKEND_URL = os.getenv("BACKEND_URL", "http://localhost:30001")  # åç«¯LLMæœåŠ¡åœ°å€

app = FastAPI(title="LLM Proxy")
client = httpx.AsyncClient(timeout=120.0)


def process_request_body(body: dict) -> dict:
    """å¤„ç†è¯·æ±‚ä½“ï¼Œè¿‡æ»¤å’Œæ›¿æ¢å…³é”®è¯"""
    body = body.copy()
    model = body.get("model", "")
    
    # é’ˆå¯¹ qwen3 æ¨¡å‹çš„ç‰¹æ®Šå¤„ç†
    if model in ["qwen3-max", "qwen3-max-thinking"]:
        # å¤„ç† response_format
        if body.get("response_format", {}).get("type"):
            body.pop("response_format", None)
        
        # æ›¿æ¢æ¶ˆæ¯å†…å®¹ä¸­çš„æ ‡ç­¾ï¼ˆä»…é’ˆå¯¹ qwen3-maxï¼‰
        if model == "qwen3-max" and isinstance(body.get("messages"), list):
            for message in body["messages"]:
                content = message.get("content")
                if isinstance(content, str):
                    # æ›¿æ¢ <think> ä¸º <thinking>
                    content = re.sub(r"<think>", "<thinking>", content, flags=re.IGNORECASE)
                    content = re.sub(r"</think>", "</thinking>", content, flags=re.IGNORECASE)
                    message["content"] = content
    
    return body


async def forward_to_backend(path: str, request: Request, body: dict = None):
    """è½¬å‘è¯·æ±‚åˆ°åç«¯æœåŠ¡ï¼ˆéæµå¼ï¼‰"""
    url = f"{BACKEND_URL}{path}"
    headers = {k: v for k, v in request.headers.items()
               if k.lower() not in ("host", "content-length")}
    
    if body is None:
        body = await request.json()
    
    try:
        response = await client.post(url, json=body, headers=headers)
        return response
    except httpx.TimeoutException:
        print(f"âš ï¸ è¯·æ±‚è¶…æ—¶: {url}")
        raise
    except httpx.RequestError as e:
        print(f"âš ï¸ è¯·æ±‚å¤±è´¥: {e}")
        raise


async def forward_to_backend_stream(path: str, request: Request, body: dict = None):
    """è½¬å‘æµå¼è¯·æ±‚åˆ°åç«¯æœåŠ¡"""
    url = f"{BACKEND_URL}{path}"
    headers = {k: v for k, v in request.headers.items()
               if k.lower() not in ("host", "content-length")}
    
    if body is None:
        body = await request.json()
    
    async def generate():
        try:
            async with client.stream("POST", url, json=body, headers=headers, timeout=120.0) as response:
                if response.status_code >= 400:
                    error_text = await response.aread()
                    try:
                        error_info = json.loads(error_text.decode())
                        yield json.dumps(error_info, ensure_ascii=False).encode()
                    except:
                        error_msg = error_text[:200] if len(error_text) > 200 else error_text
                        yield json.dumps({"error": {"message": error_msg.decode(errors="ignore")}}, 
                                       ensure_ascii=False).encode()
                    return
                
                async for chunk in response.aiter_bytes():
                    yield chunk
        except httpx.TimeoutException:
            print(f"âš ï¸ æµå¼è¯·æ±‚è¶…æ—¶: {url}")
            yield json.dumps({"error": {"message": "è¯·æ±‚è¶…æ—¶"}}, ensure_ascii=False).encode()
        except httpx.RequestError as e:
            print(f"âš ï¸ æµå¼è¯·æ±‚å¤±è´¥: {e}")
            yield json.dumps({"error": {"message": f"è¯·æ±‚å¤±è´¥: {str(e)}"}}, ensure_ascii=False).encode()
    
    return generate


@app.post("/v1/chat/completions")
async def chat_completions(request: Request):
    """èŠå¤©è¡¥å…¨ç«¯ç‚¹ - ä¸»è¦å¤„ç†é€»è¾‘"""
    body = await request.json()
    stream = body.get("stream", False)
    
    # å¤„ç†è¯·æ±‚ä½“ï¼ˆè¿‡æ»¤å’Œæ›¿æ¢å…³é”®è¯ï¼‰
    processed_body = process_request_body(body)
    
    # æµå¼å¤„ç†
    if stream:
        try:
            stream_generator_func = await forward_to_backend_stream("/v1/chat/completions", request, processed_body)
            return StreamingResponse(
                stream_generator_func(),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "X-Accel-Buffering": "no"
                }
            )
        except Exception as e:
            print(f"âš ï¸ æµå¼è¯·æ±‚å¤„ç†å¤±è´¥: {e}")
            return JSONResponse(
                content={"error": {"message": f"æµå¼è¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}"}},
                status_code=500
            )
    
    # éæµå¼å¤„ç†
    try:
        response = await forward_to_backend("/v1/chat/completions", request, processed_body)
        if response.status_code >= 400:
            error_result = response.json() if response.headers.get("content-type", "").startswith("application/json") \
                          else {"error": {"message": response.text[:200]}}
            return JSONResponse(content=error_result, status_code=response.status_code)
        
        return JSONResponse(response.json())
    except Exception as e:
        print(f"âš ï¸ éæµå¼è¯·æ±‚å¤„ç†å¤±è´¥: {e}")
        return JSONResponse(
            content={"error": {"message": f"è¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}"}},
            status_code=500
        )


@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {"status": "ok", "backend_url": BACKEND_URL}


@app.api_route("/{path:path}", methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "HEAD", "PATCH"])
async def proxy_other_endpoints(path: str, request: Request):
    """å…¶ä»–è¯·æ±‚ç›´æ¥é€ä¼ """
    url = f"{BACKEND_URL}/{path}"
    headers = {k: v for k, v in request.headers.items()
               if k.lower() not in ("host", "content-length")}
    
    try:
        if request.method == "GET":
            response = await client.get(url, headers=headers)
        else:
            body = await request.body()
            response = await client.request(request.method, url, content=body, headers=headers)
        
        # å°è¯•è¿”å› JSONï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›åŸå§‹å†…å®¹
        try:
            return JSONResponse(response.json(), status_code=response.status_code)
        except:
            return JSONResponse({"content": response.text}, status_code=response.status_code)
    except httpx.ConnectError as e:
        return JSONResponse(
            content={"error": {"message": f"æ— æ³•è¿æ¥åˆ°åç«¯æœåŠ¡ {BACKEND_URL}: {str(e)}"}},
            status_code=503
        )
    except Exception as e:
        return JSONResponse(
            content={"error": {"message": f"è¯·æ±‚å¤±è´¥: {str(e)}"}},
            status_code=500
        )


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="LLM Proxy Server")
    parser.add_argument("-port", type=int, default=30000, help="ä»£ç†æœåŠ¡ç«¯å£ (é»˜è®¤: 30000)")
    parser.add_argument("-backend", type=str, default=BACKEND_URL, help="åç«¯LLMæœåŠ¡åœ°å€")
    args = parser.parse_args()
    
    # æ›´æ–°åç«¯URL
    BACKEND_URL = args.backend
    
    print("ğŸš€ å¯åŠ¨ LLM ä»£ç†æœåŠ¡")
    print(f"   ä»£ç†åœ°å€: http://0.0.0.0:{args.port}")
    print(f"   åç«¯æœåŠ¡: {BACKEND_URL}")
    print(f"   åŠŸèƒ½: å…³é”®è¯è¿‡æ»¤å’Œæ›¿æ¢")
    
    uvicorn.run(app, host="0.0.0.0", port=args.port)
